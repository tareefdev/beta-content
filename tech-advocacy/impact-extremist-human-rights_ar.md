---
parent: tech-advocacy
title: "عالق في الشبكة: تأثير قوانين الخطاب المتطرّف على محتوى حقوق الإنسان"
date: 2019-06-12
desc: "نشرة مشتركة من: EFF، الأرشيف السوريّ، ويتنس "
image: /assets/tech-advocacy/terror-3a-big.png
---

## تنزيلات
[عالق في الشبكة: تأثير قوانين الخطاب "المتطرّف" على محتوى حقوق الإنسان](/assets/tech-advocacy/caught_in_the_net_whitepaper_2019.pdf)

## مقدمة

لطالما واجهت شركات التواصل الاجتماعي تحديّاتٍ مع ما يجب فعله حيال المحتوى المتطرف على منصاتها. ففي حين أن معظم هذه الشركات تدرج في معايير مجتمعاتها موادًا حول المحتوى "المتطرف"، إلا أن الغموض يلف تعريف هذا المحتوى في كثيرٍ من الأحيان؛ الأمر الذي يعطي واضعي السياسات ومشرفي المحتوى هامشًا كبيرًا لتحديد ما يجب إزالته وما الذي يُسمح به. للأسف، أدت الممارسات والسياسات الغامضة والفضفاضة للشركات إلى أخطاءٍ واسعة النطاق أسهمت في تقليص محتوى حقوق الإنسان.

من الخطأ الاعتقاد بأن حذف محتوىً على منصّات افتراضية يمكن أن يحل المشاكل عميقة الجذور للتطرف في المجتمعات الحديثة. توضّح الأمثلة المُساقة في هذه الوثيقة أن نصب شباكٍ كبيرة عبر الإنترنت تستخدم تقنية إدارةٍ تلقائية معيوبة؛ لا يقتصر على اصطياد محتوى يُعتبر متطرفًا، بل ويتجاوزه ليلتقط سهوًا محتوىً مفيدًا كوثائق حقوق الإنسان، ما يؤدّي بدوره إلى انكماش المجال الديمقراطي. لم يُقدّم أيٌّ من مؤيدي إدارة المحتوى التلقائية حلاً ناجعًا لهذه المشكلة.

عقب ظهور تنظيم الدولة الإسلامية في السنوات الأخيرة؛ تزايد الضغط على الشركات لاتخاذ إجراءات أكثر صرامة عندما يتعلق الأمر بهذا النوع من الخطاب. في الولايات المتحدة، جاء الضغط في صورة مقترحات تشريعيّة،(1) دعاوى مدنية من قبل ضحايا هجماتٍ إرهابية،(2) وضغوط من السلطة التنفيذية على الحكومة الفيدرالية.(3) وكثّفت المفوضية الأوروبية جهودها المبذولة في القواعد السلوكية التي أطلقتها عام 2017 والتي تطالب الشركات بمراجعة المحتوى المتطرف المُبلغ عنه خلال 24 ساعة،(4) إلى أحكامٍ أكثر صرامة من شأنها أن تغرّم الشركات بعقوبات مالية إذا فشلت في التعامل مع محتوى متطرف خلال ساعة واحدة.(5)

بغض النظر عن هذه اللوائح، فإن الغالبية العظمى من الشركات متشدّدة بالفعل عندما يتعلق الأمر بالمحتوى المتطرف. ولأنه لا يوجد تعريف متفق عليه عالميًا لمن يُعتبر إرهابيًا (إضافة إلى أن الدول ستختلف حتماً فيما إذا كان كيان معين قد استوفى هذا التعريف)، فإن الشركات التي تتخذ من الولايات المتحدة مقرًا لها، كفيسبوك، تويتر ويوتيوب، تعتمد لوائح الولايات المتحدة لتعزيز سياساتها. ونتيجة لذلك، فإن التركيز غالبًا ما يكون على الجماعات المتطرفة المُدرجة ضمن قائمة المنظمات الإرهابية الأجنبية التابعة لوزارة الخارجية الأمريكية.(6) على سبيل المثال، يوفر فيسبوك لمشرفيه قائمةً تتضمن صورًا لقادةٍ من مجموعات مدرجةٍ في تلك القائمة.(7)

على الرغم من أن الشركات تستخدم هذه القائمة كدليل، إلا أنها ليست ملزَمة بإزالة المحتوى الصادر عن هذه المجموعات بموجب القانون الأمريكي. وفقًا لما هو مُعلن، لم تتخذ حكومة الولايات المتحدة موقفًا مفاده أن السماح لمنظمة إرهابية أجنبية باستخدام منصة مجانية ومتاحة على الإنترنت هو بمثابة "توفير دعم مادي" لهذه المنظمة، الأمر الذي يُعتبر محظورًا بموجب مجموعة قوانين مكافحة الإرهاب الأمريكية. مع أن القوانين تحظر تقديم "خدمات" إلى المنظمات الإرهابية، فقد حددت المحكمة العليا في الولايات المتحدة ذلك بـ "الأفعال التي تم القيام بها لصالح أو تحت قيادة شخص آخر."(8) كما رفضت المحاكم الأمريكية باستمرار الجهود المبذولة لفرض المسؤولية المدنية على منصات الإنترنت عندما تستخدمها المنظمات الإرهابية في اتصالاتها.(9)

## إدارة المحتوى و"المحتوى المتطرف"

تُعرّف إدارة المحتوى التجاري بأنها العملية التي تتخذ المنصات من خلالها قراراتٍ بشأن المحتوى الذي يُسمح أو لا يُسمح بنشره على مواقعها، وذلك بناءً على شروط الخدمة الخاصة بها، كـ"معايير المجتمع"، أو قواعد أخرى. تعتمد هذه العملية عادةً على نظام خفارة مجتمعي، والذي يتيح لمستخدمي خدمة معينة التبليغ عن محتوىً يعتقدون أنه ينتهك القواعد. بعدها، يدخل المحتوى ضمن قائمة انتظار للإدارة، ليقوم مشرفٌ بشريّ بتحديد ما إذا كان ذلك المحتوى ينتهك القواعد بالفعل أم لا. تؤدي الانتهاكات المتكررة في معظم المنصات الأساسية إلى "عقوبات"، حيث يُحظر المستخدم مؤقتًا لفترة زمنية متزايدة.

يتزايد اليوم الاعتماد على "التبليغ التلقائي" لإدارة المحتوى المتطرف، وهي عملية تستخدم فيها المنصات أدوات مملوكة لها للكشف التلقائي عن محتوى يحتمل أن يشمل انتهاكات ما، ليُراجع بعد ذلك من قبل مشرفٍ بشريّ. غالبًا ما يتم ذلك قبل أن يشاهد مستخدمو المنصّة هذا المحتوى.

تزايد استخدام الشركات لخوارزميات التعلم الآلي بشكل كبير في السنوات الأخيرة. في عالم الحوسبة، الخوارزميات هي مجموعة من التعليمات لتنفيذ شيء ما. خوارزميات التعلم الآلي هي خوارزميات تُعطى مجموعة أولية من البيانات وبعض القواعد، ثم تتعلم وتتطور عندما تحتّك بمزيد من البيانات. من أجل تدريب خوارزمية التعلم الآلي، يجب على الشركة إنشاء مجموعة بيانات تتضمن كمية كبيرة من المحتوى تحت تصنيف واحد، لتغذّي بها الخوارزمية بغرض التدريب. على سبيل المثال، من أجل تحديد المحتوى المتطرف بدقة، ستنشئ شركة مثل يوتيوب مجموعة من البيانات التي تعرّفها على أنها متطرفة - مثل عدد كبير من مقاطع فيديو داعش - ثم تلقّم تلك البيانات للخوارزمية الخاصة بها.

يصعب أحيانًا تحديد سبب ارتكاب الخوارزمية لبعض الأخطاء، كما ستوضّح بعض الحالات أدناه. إذ لا يمكن فهم خوارزميات التعلم الآلي من قبل البشر ما لم تُصمّم خصيصًا لتكون "قابلة للتفسير". حيث لا يمكننا أن نرى "عمليات التفكير" الخاصة بهذه الخوارزميات نظرًا لأنها تتعلم وتتطور من تلقاء ذاتها دون الحاجة إلى مشاركة بشرية. لسوء الحظ، تستخدم جميع المنصات خوارزميات تعلم آلي مغلقة المصدر ومحمية بموجب قوانين الملكية الفكرية من أيّة مراجعة خارجية. في واقع الأمر، مُنع المجتمع المدني والحكومات من الوصول إلى بيانات التدريب أو الافتراضات الأساسية التي تقود الخوارزميات، ولم يكن هناك أي نوع من التدقيق من قبل طرف ثالثٍ لهذه التقنية.

ازدادت حدّة هذه المشكلة أكثر مع إدخال قواعد بيانات التجزئة Hashes لتتبع المحتوى المتطرف وإزالته. يمكن اعتبار الـ Hashes بمثابة "بصمات أصابعٍ" رقميّة للمحتوى؛ تستخدمها الشركات لتحديد نوع المحتوى وإزالته من المنصّات الخاصة بها عند اللزوم. وحيث أنها فريدة من نوعها؛ فهي تتيح كشف هوية محتوى معيّن بسهولة. عندما يتم تحديد صورة على أنها "محتوى إرهابي"؛ تُوسَم بعلامة تجزئة Hash وتُدخَل في قاعدة بيانات، مما يسمح بالتعرّف على أية عمليات تحميل مستقبلية لنفس الصورة بسهولة.

و رغم استخدام قواعد بيانات مماثلة لتتبع وإزالة صور استغلال الأطفال من منصات التواصل الاجتماعي بنجاح في السابق، فإن قواعد البيانات هذه - التي يديرها المركز الوطني للأطفال المفقودين والمستغلين (NCMEC) والمركز الدولي للأطفال المفقودين والمستغلين (ICMEC) - تعمل بوجود رقابة على آليات إنفاذ القانون، وليس لدى أيّ من الشركات أو الموظفين المكلفين بإنفاذ القانون إمكانية الوصول المباشر إلى الصور الموجودة في قاعدة البيانات.

استخدم المنتدى العالمي للإنترنت لمكافحة الإرهاب (GIFCT)، وهي مبادرة صناعة أنشأها كلٌ من فيسبوك، مايكروسوفت، تويتر ويوتيوب، تقنيةً مماثلة لاستحداث قاعدة بيانات للصور "الإرهابية" عام 2016. تمت مشاركة قاعدة البيانات بين الشركات الأعضاء في منتدى GIFCT، والذي يضم شركاتٍ أصغر لا تملك الموارد اللازمة لبناء قواعد البيانات الخاصة بها. لكن المحصلة هي استخدام قاعدة بيانات واحدة على نطاق واسع عبر الإنترنت، وبالتالي توسع وتضاعف الأخطاء.

لا تُشارك قاعدة بيانات GIFCT مع أيّ من أعضاء المجتمع المدني المهتمّين بحقوق الإنسان، شأنها في ذلك شأن محتوى تدريب خوارزميات المحتوى المتطرف. كما أن موقع GIFCT لا يُقدّم سوى معلومات قليلة حول كيفية عمله. خلافًا لقواعد البيانات التي تديرها NCMEC و ICMEC، تعمل قاعدة بيانات GIFCT دون رقابة خارجية. بدلاً من ذلك، فإن تحديد ما يُعتبر متطرفًا أمرٌ متروك للشركات.

من المهم أن نعود إلى الأرقام لندرك حجم المشكلة. يوضح تقرير الشفافية من جوجل أن يوتيوب أزال 33 مليون مقطع فيديو عام 2018،(10) ما يعني حوالي 90000 مقطع يوميًا. 73٪ من المقاطع التي حُددت لاحتوائها انتهاكًا محتملًا لشروط الخدمة أُزيلت من خلال عمليات تلقائية قبل أن تُتاح للعرض. أزال فيسبوك ما يقارب 15 مليون وحدة محتوى اعتُبرت أنها تحمل "أجندة إرهابية" بين أكتوبر 2017 وأكتوبر 2018. نشرت الشركة أنه في الربع الثالث من عام 2018 وجد فيسبوك أن "99.5٪ من المحتوى [أُزيل] قبل أن يُبلغ المستخدمون عنه؛ في حين أبلغ المستخدمين عن الـ 0.5٪ المتبقية".(11) أما تويتر، الذي أزال 1.2 مليون حساب مرتبط بالإرهاب بين 2015 والربع الأخير من عام 2017،(12) فقد أزال 166،153 حسابًا إضافيًا لنشرها محتوى إرهابي في النصف الثاني من عام 2018.(13)

## تأثير التدابير الصارمة على المستخدمين المهمّشين

تزيل منصات وسائل التواصل الاجتماعي المحتوى وفق تصنيفاتٍ مختلفة عن طريق الخطأ، لكنّ أيًا من تلك الشركات لا تنشر معدل الخطأ الخاص بها. نتيجة لذلك، قد يكون من الصعب فهم أين ومتى وكيف يعاني المستخدمون من عمليات الإزالة غير الدقيقة والخاطئة. على أية حال، توضح الأمثلة أدناه أنه من الصعب على المشرفين البشر - ومن المستحيل بالنسبة للآلات - التمييز باستمرار بين أنشطة المناصرة، الخطاب المضاد، السخرية من التطرف، وبين التطرف نفسه. لا شك في أن أنظمة إدارة المحتوى الصارمة ترتكب أخطاء على نحو واسع، والمستخدمون المهمشون هم من يدفع ثمن هذه الأخطاء.

### مثال 1: استقلال الشيشان

في عام 2017، أزيلت من فيسبوك مجموعة تدعى "استقلال الشيشان!" تدافع عن استقلال جمهورية إيسكريا الشيشانية؛ وذلك بسبب انتهاكها لمعايير مجتمع الشركة التي تحظر "المنظمات المتورطة في نشاط إرهابي أو نشاط إجرامي منظّم"، على الرغم من أن الأدلّة التدريبية تحدّد جمهورية إيسكريا الشيشانية بأنها "لا تنتهك" القواعد. قال متحدث باسم فيسبوك أن الحذف تم خطأً وأن الشركة "تخطئ الأمور أحيانًا".(14)

### مثال 2: النشاط الكردي

غالبًا ما تكون الجماعات التي تدافع عن استقلال كردستان هدفًا للمعايير الفضفاضة لإدارة المحتوى، على الرغم من أن مجموعة واحدةً فقط - حزب العمال الكردستاني (PKK) - تُعتبر منظمة إرهابية من قبل الحكومات. ومن ثمّ، فإن انتقاد المجموعة وإدانتها مسموح على فيسبوك، لكن الأمر لا يسير على هذا النحو.(15)

وفقًا لـ هيومن رايتس ووتش؛ يعتبر الأكراد أهدافًا متكررة لانتهاكات حقوق الإنسان من قبل الحكومة التركية.(16) الحكومة التركية هي الأسوأ في سجن الصحفيين عالميًا.(17) كما أنها رائدة في مطالب الرقابة، حيث تطالب الشركات بإزالة أي محتوى غير قانوني في البلاد، بما في ذلك انتقاد مؤسسها، أتاتورك.

ادّعى ناشطون أكراد أن فيسبوك قد أزال مرارًا منشورات لهم لا تنتهك معايير المنصة. (18) كما وصف سياسي كردي أُغلقت صفحته ما يحدث بأنه "تحالف قذر" بين الحكومة التركية وفيسبوك، مشيرًا إلى أن الحزب الحاكم في تركيا، حزب العدالة والتنمية، شارك صور زعيم حماس (المدرج أيضًا ضمن قائمة الحكومة الأمريكية للمنظمات الإرهابية الأجنبية) ولم تُتّخذ أية إجراءات حيال ذلك. (19) عندما يُزال محتوى متعلّق بمعارضة مشروعة، مثل ما ينشره نشطاء أكراد، من قبل شركة، سواء عن طريق الخطأ أو نتيجة ضغط حكومي، فإن الشركة عمليًا منحازة في نزاع سياسي.

### مثال 3: التعليقات الساخرة

عام 2017، أزال فيسبوك صورة نشرها أحد الصحفيين الإماراتيين البارزين لزعيم حزب الله حسن نصر الله مغطاة بقوس قزح علم فخر المثليين.(20) كان المنشور تعليقًا على شعبية حزب الله بين شريحة معينة من اليسار السياسي على الرغم من عدم دعمه لحقوق مجتمع إل جي بي تي، لكن ذلك كان غامضًا للغاية بالنسبة لمشرفي المحتوى الموجَّهين لإزالة معظم الصور التي تحتوي على وجوه قادةٍ إرهابيين معروفين.(21)

## وسائل التواصل الاجتماعي كدليلٍ وذاكرة

يعد توثيق انتهاكات حقوق الإنسان عبر وسائل التواصل الاجتماعي أمرًا حاسمًا لجهود العدالة والمساءلة، وفي بعض الحالات يكون بمثابة ذاكرة جماعية. إن مقاطع الفيديو والنصوص المنشورة على الإنترنت تُعتبر تاريخًا حيًا لبعض مجتمعات الشتات، كما أن هذه الوثائق توفّر أحيانًا الدليل الوحيد على ارتكاب جريمة. ومع ذلك، تؤدي سياسات إدارة محتوى وسائل التواصل الاجتماعي المرتبطة بالتطرف إلى حذف وثائق هامّة في كثير من الحالات. من المستحيل تقريبًا استعادة المحتوى المحذوف بطريقة خاطئة إذا كان الشخص الذي نشره قد توفّي، اعتُقل، أو لم يكن لديه وصول إلى البريد الإلكتروني، وهي جميعها مشكلات شائعة في مناطق النزاع.

### رقابة يوتيوب على وثائق الصراع في سوريا، اليمن وأوكرانيا

غالبًا ما يستخدم المدافعون عن حقوق الإنسان في سوريا منصات وسائل التواصل الاجتماعي لنشر وثائق النزاع وتعميمها، إذ يمتلكون القدرة على استخدام هذه الوسائل بفعالية واستمرار. في مقابلة مع مجموعات في بداية الانتفاضة السورية، قال نُشطاء:

> كانت نقطة انعطاف عندما أدركنا أننا يجب أن نبدأ في تنظيم أنفسنا، يجب أن نبدأ بعملٍ منظّم. نظرًا لأن جميع القنوات الإعلامية رفضت نشر هذا النوع من مقاطع الفيديو... لم يكن هناك في الواقع أية تغطية إعلامية، لم يكن لدينا سوى قناة واحدة ووسائل التواصل الاجتماعي، يوتيوب وفيسبوك فقط... تعاون الشباب مع القنوات وأنشؤوا المزيد منها على يوتيوب... كانت هذه أوائل المجموعات المحلية المعتمدة على يوتيوب. كانوا منظّمين، وكان لديهم مراسلون في كل مكان، وجمعوا مقاطع الفيديو.... أول ظاهرة منظمة في سوريا كانت مجموعة إعلامية. (22)

يفوق عدد ساعات مقاطع الفيديو التي توثّق النزاع في سوريا عدد ساعات النزاع نفسه. ومع استمرار تحميل أكثر من 50 مقطع فيديو على يوتيوب يوميًا، فإنه يمكن القول إن الحالة السورية أتاحت لأيّ شخص أن يكون شاهدًا على النزاع وقت وقوعه، ربما للمرة الأولى في التاريخ.(23)

استخدم يوتيوب تقنية تبليغ تلقائية عبر خوارزميات التعلم الآلي لإزالة آلاف من قنوات يوتيوب السورية التي تنشر مقاطع فيديو توثق انتهاكات حقوق الإنسان. بما فيها قنوات مثل المرصد السوري لحقوق الإنسان، مركز توثيق الانتهاكات، شبكة شام للأنباء، ومركز حلب الإعلامي. تتراوح نشاطات حسابات وسائل التواصل الاجتماعي المُزالة بين توثيق الاحتجاجات في سوريا إلى الإبلاغ غير التقليدي عن هجمات عنيفة، لكنّ أيًا منها لم يحرّض على العنف أو يشجّع على أنشطة خطرة. (24)

ما بين عام 2011 وحتى مايو/ أيار 2019، ما لا يقل عن 206،077 مقطع فيديو يوثق انتهاكات حقوق الإنسان لم يعد متاحًا على يوتيوب.(25) يشمل ذلك 381 مقطع فيديو يوثق غارات جويّة استهدفت مستشفيات أو منشآت طبية. أحد هذه الفيديوهات بعنوان "طفس: قصف مدفعي عنيف على المستشفى الوطني 11/8/2012" حيث يُظهر قوات الحكومة السورية تقصف الطاقم الطبي والمرضى داخل المستشفى .(26)

تُشاهَد أمثلة مشابهة في اليمن، حيث أدت الحرب بين التحالف الذي تقوده السعودية والحوثيين منذ عام 2015 إلى القتل المباشر لما يقدر بـ 70،000 شخص،(27) نزوح أكثر من ثلاثة ملايين شخص،(28) ووفاة نحو 85،000 طفل من الجوع.(29) نتيجة لسياسات إدارة المحتوى فإن مقاطع فيديو لم تعد متاحة على يوتيوب؛ من بينها فيديو بعنوان "اللحظة الأولى لقصف القاعة الكبرى في صنعاء 08/10/2016" (30) ومقطع فيديو آخر بعنوان "مجزرة سعودية تستهدف مخيمات النازحين في المزرق" (31).

في أوكرانيا التي عمّتها الحرب منذ أن ضمّت روسيا شبه جزيرة القرم عام 2014، أُزيلت مقاطع فيديو من يوتيوب توثق تسليح القوات الموالية لروسيا والقوات المناهضة للحكومة. أحد الأمثلة بعنوان "المعدات العسكرية التي قدمتها روسيا إلى دونباس." (32)

### الأدلّة التي توفرها وسائل التواصل الاجتماعيّ والسوابق القضائية

تعيق إدارة المحتوى جهود حقوق الإنسان للمساءلة القانونية. إذ يمكن أن تقدم وسائل التواصل الاجتماعي أدلة لا يمكن الاستغناء عنها من مناطق النزاع، لا سيما في الأماكن التي يواجه فيها الصحفيون الأجانب، المنظمات غير الحكومية ووكالات المراقبة الدولية صعوبات في الوصول إلى البلاد لتوثيق انتهاكات الحقوق.(33) في حين أن المحاكم ومجموعات التوثيق التقليدية تتوانى عن استغلال هذه الأدلة عادةً، إلا أن سوابق قضائية يحتلّ فيها محتوى وسائل التواصل الاجتماعيّ مكانة بارزة بدأت بالظهور.

في عام 2016، على سبيل المثال، رُفعت دعوى قضائية ضد معارض سوريّ سابق شارك في قتل سبعة جنود سوريين عقب أسرهم. اعتمدت المحكمة على محتوى منشور على فيسبوك وتويتر لتحديد توقيت ومكان أسر الجنود، فضلاً عن التوصل لحقيقة أن 41 ساعة فقط فصلت بين أسرهم وإعدامهم. تواصل المدّعون العامّون مع فيسبوك للتحقق من البيانات الوصفية للمحتوى. (34)

في عام 2017، ومرة ​​أخرى في 2018، أصدرت المحكمة الجنائية الدولية (ICC) مذكّرة توقيف بحق المواطن الليبي محمود مصطفى بوسيف الورفلي. حيث اتُهم بالمسؤولية المباشرة عن قتل 33 شخصًا أو عن إصدار أمر بإعدام هؤلاء الأشخاص. ينص أمر التوقيف على أن الأدلة المرتبطة بسبعة من هذه الحوادث استندت إلى حد كبير على مقاطع فيديو ونصوص مقاطع فيديو نُشرت على حسابات التواصل الاجتماعي الخاصة بالورفلي. (35)

## خاتمة

عند وقوع أحداث مأساوية مثل هجوم كرايستشيرش، يصبح الدافع للاستجابة ملحًا لدرجة أنه قد يؤدي إلى نتائج غير مقصودة. في الأشهر الأخيرة، تراجعت العديد من الشركات التي وقّعت على دعوة كرايستشيرش ضد المقترحات التشريعية لطلبات الإزالة الآلية أمام واضعي السياسات. في حين أن هذه الشركات تقدم وعودًا كبيرة حول إدارة المحتوى التلقائية، فإنها تقر بأنّ هذه التقنية ليست مضمونة. يعترف فيسبوك بوجود معدّل خطأ مرتفع حتى مع وجود المراجعة البشرية.(36) لسوء الحظ، لا تؤثر إدارة المحتوى على جميع الفئات بشكل متساوٍ، كما توضح العديد من الأمثلة أعلاه، لكن لديها الإمكانية على زيادة حرمان المجتمعات المهمّشة من حقوقها.

إن الانسياق خلف حلول بسيطة لمعالجة مشكلة التطرف المعقّدة عبر الإنترنت جذّاب بالفعل، لكن لا يجب على الحكومات والشركات على حد سواء أن تتسرّع في الاندفاع نحو الحلول التي تمسّ بحرية التعبير، الحق في التجمع، وحق الوصول إلى المعلومات.

## حول المؤلفين

### مؤسسة التخوم الإلكترونية EFF

 مؤسسة التخوم الإلكترونية هي مؤسسة غير ربحية رائدة في مجال الدفاع عن الخصوصية الرقمية، وحرية التعبير والابتكار. تأسست شركة EFF عام 1990، لتدعم خصوصية المستخدم، حرية التعبير والابتكار من خلال التقاضي، تحليل السياسات، النشاط الشعبي وتطوير التقنية. كما تعمل على حماية وتعزيز كلٍّ من حقوق الإنسان والحقوق الدستورية فيما يزداد الاعتماد على التكنولوجيا.

### الأرشيف السوري

الأرشيف السوري منظمة تكرس جهودها لصون، حفظ، وإضافة قيمة إلى الوثائق المعرضة للخطر المتعلقة بإنتهاكات حقوق الإنسان المُرتبكة من قبل جميع الأطراف خلال النزاع في سوريا. يستخدم الأرشيف السوري هذا المحتوى لدعم جهود العدالة والمساءلة، ويتواصل مع منصات وسائل التواصل الاجتماعي لمساعدة مجموعات التوثيق على استرداد محتوى حقوق الإنسان الذي تمت إزالته عن غير قصد بسبب سياسات إدارة المحتوى.

### ويتنس

منظمة ويتنس هي منظمة دولية غير ربحية تستخدم التعليم والمناصرة لضمان استخدام الفيديو بشكل أخلاقيّ، آمن وفعّال من قبل المدافعين عن حقوق الإنسان أثناء كفاحهم. تقدم ويتنس دورات تدريبية ومواد عن الدعوة عبر الفيديو والفيديو كدليل والأرشفة وغير ذلك. يركز برنامج WITNESS'Tech + Advocacy على الحاجة إلى ضمان دعم المدافعين عن حقوق الإنسان، بدلاً من إلحاق الأذى بهم، من قبل أدوات وسياسات شركات التكنولوجيا من خلال مناصرة السياسات وجهود المساءلة في المنصة.


---

## حواشٍ

1. Kelsey Harclerode, “Mandatory Reporting of User Content Chills Speech and Violates Privacy Rights,” Electronic Frontier Foundation, 5 August 2015, https://www.eff.org/deeplinks/2015/08/mandatory-reporting-user-content-chills-speech-and-violates-privacy-rights.

2. Aaron Mackey, “EFF to Court: Holding Twitter Responsible for Providing Material Support to Terrorists Would Violate Users’ First Amendment Rights,” Electronic Frontier Foundation, 8 June 2017, https://www.eff.org/deeplinks/2017/06/eff-court-holding-twitter-responsible-providing-material-support-terrorists-would.

3. CBS News, “The Delicate Balance Fighting ISIS Online,” 20 February 2015, https://www.cbsnews.com/news/world-governments-try-to-shut-down-isis-social-media-propaganda-operations/.

4. Amar Toor, “Facebook, Twitter, Google, and Microsoft agree to EU hate speech rules,” the Verge, 31 May 2016, https://www.theverge.com/2016/5/31/11817540/facebook-twitter-google-microsoft-hate-speech-europe.

5. Colin Lecher, “Aggressive new terrorist content regulation passes EU vote,” the Verge, 17 April 2019, https://www.theverge.com/2019/4/17/18412278/eu-terrorist-content-law-parliament-takedown.

6. قائمة المنظمات الإرهابية الأجنبية التابعة لوزارة الخارجية الأمريكية, https://www.state.gov/foreign-terrorist-organizations/ (accessed 29 May 2019).

7. The Guardian, “How Facebook Guides Moderators on Terrorist Content,” 24 May 2017, https://www.theguardian.com/news/gallery/2017/may/24/how-facebook-guides-moderators-on-terrorist-content.

8. Holder v. Humanitarian Law Project, 561 U.S. 1 (2010).

9. انظر، على سبيل المثال, Fields v. Twitter, 881 F.3d 739 (9th Cir. 2018).

10. تقرير شفافية جوجل, “YouTube Community Guidelines enforcement,” https://transparencyreport.google.com/youtube-policy/removals (تم الوصول له في 12 مايو/أيار 2019).

11. فيسبوك, “Community Standards Enforcement Report,” https://transparency.facebook.com/community-standards-enforcement#terrorist-propaganda (accessed 12 May 2019).

12. Don Reisinger, “Twitter Has Suspended 1.2 Million Terrorist Accounts Since 2015,” Fortune, 5 April 2018, http://fortune.com/2018/04/05/twitter-terrorist-account-suspensions/.

13. Foo Yun Chee, “Twitter suspended 166,153 accounts for terrorism content in second half 2018,” Reuters. 9 May 2019, https://www.reuters.com/article/us-twitter-security/twitter-suspended-166153-accounts-for-terrorism-content-in-second-half-2018-idUSKCN1SF1LN.

14. Julia Carrie Wong, “Facebook blocks Chechnya activist page in latest case of wrongful censorship,” the Guardian, 6 June 2017, https://www.theguardian.com/technology/2017/jun/06/facebook-chechnya-political-activist-page-deleted.

15. The Guardian, “How Facebook Guides Moderators on Terrorist Content.” https://www.theguardian.com/news/gallery/2017/may/24/how-facebook-guides-moderators-on-terrorist-content

16. هيومن رايتس ووتش, “تركيا، أحداث 2017" https://www.hrw.org/world-report/2018/country-chapters/turkey (تم الوصول لها في 29 مايو/أيار 2019).

17. Elana Beiser, “Hundreds of journalists jailed globally becomes the new normal,” Committee to Protect Journalists, 13 December 2018, https://cpj.org/reports/2018/12/journalists-jailed-imprisoned-turkey-china-egypt-saudi-arabia.php.

18. Sara Spary, “Facebook Is Embroiled In A Row With Activists Over "Censorship," Buzzfeed, 8 April 2016, https://www.buzzfeed.com/saraspary/facebook-in-dispute-with-pro-kurdish-activists-over-deleted.

19. Hurriyet Daily News, Kurdish politicians to take action after Facebook admits to banning pages with PKK content,” 29 April 2013, http://www.hurriyetdailynews.com/kurdish-politicians-to-take-action-after-facebook-admits-to-banning-pages-with-pkk-content-53465.

20. Sophia Cope, Jillian C. York, and Jeremy Gillula, “Industry Efforts to Censor Pro-Terrorism Online Content Pose Risks to Free Speech,” Electronic Frontier Foundation, 12 July 2017, https://www.eff.org/deeplinks/2017/07/industry-efforts-censor-pro-terrorism-online-content-pose-risks-free-speech.

21. The Guardian, “How Facebook Guides Moderators on Terrorist Content.” https://www.theguardian.com/news/gallery/2017/may/24/how-facebook-guides-moderators-on-terrorist-content

22. Revolutionary Echoes from Syria, (Hourriya, 2016), 18-21. Audio available at: https://archive.org/details/RevolutionaryEchoesFromSyria.

23. Armin Rosen, “Erasing History: YouTube’s Deletion Of Syria War Videos Concerns Human Rights Groups,” Fast Company, 7 March 2018, https://www.fastcompany.com/40540411/erasing-history-youtubes-deletion-of-syria-war-videos-concerns-human-rights-groups.

24. الأرشيف السوري, https://syrianarchive.org/ar/tech-advocacy/ (تم الوصول لها في 29 مايو/ أيار 2019).

25. Ibid.

26. “طفس: قصف مدفعي عنيف على المستشفى الوطني 11/8/2012”, مقطع فيديو لم يعد متاحًا على يوتيوب available, https://www.youtube.com/watch?v=ipaaQGqtfTk.

27. Al Jazeera, “More than 70,000 killed in Yemen's civil war: ACLED,” 19 April 2019, https://www.aljazeera.com/news/2019/04/yemen-war-death-toll-reaches-70000-report-190419120508897.html.

28. United Nations Agency for Refugees and Migrants, “More than 3 million displaced in Yemen – joint UN agency report,” 22 August 2016, https://refugeesmigrants.un.org/more-3-million-displaced-yemen-%E2%80%93-joint-un-agency-report

29. Sam Magdy, “Save the Children says 85,000 kids may have died of hunger in Yemen,” USA Today, 21 November 2018, https://eu.usatoday.com/story/news/world/2018/11/21/yemen-children-hunger/2076683002/.

30. “اللحظة الأولى لقصف القاعة الكبرى في صنعاء 08/10/2016” مقطع فيديو لم يعد متاحًا على يوتيوب, https://www.youtube.com/watch?v=dNax-YKBLNE.

31. “مجزرة سعودية تستهدف مخيمات النازحين في المرزق” مقطع فيديو لم يعد متاحًا على يوتيوب, https://www.youtube.com/watch?v=_Z5peAj4kTo.

32. “Military equipment supplied by Russia to the Donbass,” YouTube video no longer available, https://www.youtube.com/watch?v=CJm5bjM3Z5c.

33. Elliot Higgins, “Weapons From the Former Yugoslavia Spread Through Syria’s War,” The New York Times, 25 February 2013, http://atwar.blogs.nytimes.com/2013/02/25/weapons-from-the-former-yugoslavia-spread-through-syrias-war/.

34. Christina Anderson, “Syrian Rebel Gets Life Sentence for Mass Killing Caught on Video,” The New York Times, 16 February 2017, https://www.nytimes.com/2017/02/16/world/europe/syrian-rebel-haisam-omar-sakhanh-sentenced.html.

35. The Prosecutor v. Mahmoud Mustafa Busayf Al-Werfalli, Case No.ICC-01/11-01/17-2, Warrant of Arrest (15 August 2017), https://www.icc-cpi.int/Pages/record.aspx?docNo=ICC-01/11-01/17-2.

36. Ariana Tobin, Madeleine Varner, and Julia Angwin. “Facebook Uneven Enforcement of Hate Speech Rules Allows Vile Posts to Stay Up,” ProPublica, 28 December 2017, https://www.propublica.org/article/facebook-enforcement-hate-speech-rules-mistakes.
